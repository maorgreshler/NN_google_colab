{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSZscpuvyWv+oAnHibtzCh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maorgreshler/NN_google_colab/blob/main/NN_shared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCsDPej27oQX"
      },
      "outputs": [],
      "source": [
        "import torc\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Disable oneDNN optimizations\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "\n",
        "# Import TensorFlow after setting the environment variable\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Define the dataset class\n",
        "class TrajectoryDataset(Dataset):\n",
        "    def __init__(self, csv_file, input_columns, target_columns, scaler=None):\n",
        "        data = pd.read_csv(csv_file)\n",
        "\n",
        "        self.inputs = data[input_columns].values.astype('float32')  # Select input columns\n",
        "        self.targets = data[target_columns].values.astype('float32')  # Select target columns\n",
        "\n",
        "        # Normalize the data\n",
        "        if scaler is None:\n",
        "            self.input_scaler = StandardScaler().fit(self.inputs)\n",
        "            self.target_scaler = StandardScaler().fit(self.targets)\n",
        "        else:\n",
        "            self.input_scaler, self.target_scaler = scaler\n",
        "\n",
        "        self.inputs = self.input_scaler.transform(self.inputs)\n",
        "        self.targets = self.target_scaler.transform(self.targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "\n",
        "# Training function with validation and TensorBoard logging\n",
        "def train_model(csv_file, input_columns, target_columns, num_epochs=20,hidden_size=128, batch_size=64, learning_rate=0.001, validation_split=0.2,model_name=\"model.pth\"):\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load dataset and create scalers\n",
        "    full_dataset = TrajectoryDataset(csv_file, input_columns, target_columns)\n",
        "    dataset_size = len(full_dataset)\n",
        "    val_size = int(validation_split * dataset_size)\n",
        "    train_size = dataset_size - val_size\n",
        "\n",
        "    # Split dataset into training and validation sets\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize model, loss function, and optimizer\n",
        "    #model = SLP(input_size=len(input_columns), output_size=len(target_columns),layer_width=1000).to(device)\n",
        "    model = MLP(input_size=len(input_columns),hidden_size=hidden_size, output_size=len(target_columns)).to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "    # TensorBoard writer\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Calculate average losses\n",
        "        train_loss /= train_size\n",
        "        val_loss /= val_size\n",
        "\n",
        "        # Log to TensorBoard\n",
        "        writer.add_scalars('Loss', {'Train': train_loss, 'Validation': val_loss}, epoch)\n",
        "\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    writer.close()\n",
        "    return model, (full_dataset.input_scaler, full_dataset.target_scaler)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == '__main__':\n",
        "    csv_file = r'C:\\Users\\matan\\OneDrive\\Desktop\\Matan\\project A\\ProjectA_col\\dataGen\\overfit_test.csv'  ##כאן צריך להכניס את המיקום של תקיית הדתא איתה אתה מאמן\n",
        "\n",
        "    # Define input and target columns (D to K and L to T)\n",
        "    input_columns = ['velocity', 'yaw_angle', 'yaw_rate', 'slip_angle', 'front_wheel', 'back_wheel', 'acceleration', 'angular_acceleration']  # D to K\n",
        "    target_columns = ['delta_x_position', 'delta_y_position', 'delta_theta', 'delta_velocity', 'delta_yaw_angle', 'delta_yaw_rate', 'delta_slip_angle', 'delta_front_wheel', 'delta_back_wheel']  # L to T\n",
        "    model_name = \"overfit2.pth\"## כאן להכניס את השם שרוצים לשמור של שאנחנו רוצים לרשת\n",
        "    model, scalers = train_model(csv_file, input_columns, target_columns,100,1000,model_name=model_name)#כאן צריך אולי לרשום את הערכים של הפרמטרים אותם אנחנו רוצים להכניס לרשת\n",
        "    os.chdir(r'C:\\Users\\matan\\OneDrive\\Desktop\\Matan\\project A\\ProjectA_col\\NN')#change acrodinly\n",
        "    torch.save(model.state_dict(), model_name)\n",
        "    print(\"Script executed in:\", os.getcwd())\n"
      ]
    }
  ]
}